{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the output of the wayveslam_visual_odometry step\n",
    "After switching from using the intrinsics database intrinsics to using the corrected calibration table intrinsics, we want to make sure that the vo trajectories are still reasonable. For this we simply compare the current vo trajectory on databricks with the ones that were generated using the corrected calibration table intrinsics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wayve.services.data.lakehouse.common.external_access.wayve_delta_table import WayveDeltaTable\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, no_type_check\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import json\n",
    "from bokeh.io import output_file, save\n",
    "\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from wayve.core.ai.geometry.trajectory import Trajectory3D\n",
    "from wayve.core.ai.geometry.transform_3d import Transform3D\n",
    "\n",
    "\n",
    "from wayve.core.ai.geometry.frame_reference import SerializedFrameRef\n",
    "from wayve.core.ai.geometry.rotation_3d import RotationVector3D\n",
    "from wayve.core.ai.geometry.vector_3d import Vector3D\n",
    "from wayve.core.data.schema.state.wayveslam import SensorPoseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VO_MAIN_CLOUD_TABLE = \"\"\n",
    "VO_BRANCH_CLOUD_TABLE = \"\"\n",
    "\n",
    "OUT_DIR = \"/mnt/remote/data/benjin/tmp_share/notebooks/step_testing/visual_odometry_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vo_df(table_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get VO poses from delta tables for run ID. Renames transform columns to WayveSLAM convention\n",
    "    \"\"\"\n",
    "    df = WayveDeltaTable(path=table_path).to_pandas_dataframe()\n",
    "    return df\n",
    "\n",
    "def sensorpose_to_transform3d(sensorpose_df: pd.DataFrame) -> Transform3D:\n",
    "    \"\"\"\n",
    "    Convert sensorpose dataframe to Transform3D\n",
    "    \"\"\"\n",
    "    rotation = R.from_quat(\n",
    "        np.stack(\n",
    "            [\n",
    "                sensorpose_df[SensorPoseSchema.quaternion_x.short_field_name],\n",
    "                sensorpose_df[SensorPoseSchema.quaternion_y.short_field_name],\n",
    "                sensorpose_df[SensorPoseSchema.quaternion_z.short_field_name],\n",
    "                sensorpose_df[SensorPoseSchema.quaternion_w.short_field_name],\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    translation = Vector3D(\n",
    "        torch.from_numpy(\n",
    "            np.stack(\n",
    "                [\n",
    "                    sensorpose_df[SensorPoseSchema.x_position_m.short_field_name],\n",
    "                    sensorpose_df[SensorPoseSchema.y_position_m.short_field_name],\n",
    "                    sensorpose_df[SensorPoseSchema.z_position_m.short_field_name],\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    batch_shape = translation.batch_shape\n",
    "    source_frame = sensorpose_df[SensorPoseSchema.source_frame.short_field_name].to_numpy()\n",
    "    destination_frame = sensorpose_df[SensorPoseSchema.destination_frame.short_field_name].to_numpy()\n",
    "\n",
    "    return Transform3D(\n",
    "        translation=translation,\n",
    "        rotation=RotationVector3D(torch.from_numpy(rotation.as_rotvec())),\n",
    "        from_frame=SerializedFrameRef.from_frame_ref(source_frame, batch_shape),\n",
    "        to_frame=SerializedFrameRef.from_frame_ref(destination_frame, batch_shape),\n",
    "    )\n",
    "\n",
    "def sensorpose_to_trajectory3d(sensorpose_df: pd.DataFrame) -> Trajectory3D:\n",
    "    \"\"\"\n",
    "    Convert sensorpose dataframe to Trajectory3D\n",
    "    \"\"\"\n",
    "    transforms = sensorpose_to_transform3d(sensorpose_df)\n",
    "    timestamps = sensorpose_df[\"timestamp_unixus\"].to_numpy()\n",
    "    return Trajectory3D(\n",
    "        timestamps_us=torch.from_numpy(timestamps),\n",
    "        transforms=transforms,\n",
    "    )\n",
    "\n",
    "@no_type_check\n",
    "def plot_multiple_trajectories(plot_source: Dict[str, Trajectory3D], title: str) -> figure:\n",
    "    \"\"\"Plots a dict of trajectories with no legends\"\"\"\n",
    "    bev_plot = figure(match_aspect=True)\n",
    "    colors = Category10[10]\n",
    "\n",
    "    for i, key in enumerate(plot_source):\n",
    "        bev_plot.line(\n",
    "            x=(plot_source[key].translation.forward_left_up).numpy().T[0],\n",
    "            y=(plot_source[key].translation.forward_left_up).numpy().T[1],\n",
    "            line_width=2,\n",
    "            color=colors[i % len(colors)],\n",
    "            legend_label=key,\n",
    "        )\n",
    "\n",
    "    bev_plot.title = title\n",
    "    return bev_plot\n",
    "\n",
    "def plot_trajectory_alignment(\n",
    "    orig_poses: Trajectory3D,\n",
    "    new_poses: Trajectory3D,\n",
    "    output_path: Path,\n",
    "):\n",
    "    # PLOTTING\n",
    "    fig = plot_multiple_trajectories(\n",
    "        title='compare',\n",
    "        plot_source={\n",
    "            'vo_original': orig_poses,\n",
    "            'vo_new': new_poses,\n",
    "        }\n",
    "    )\n",
    "    output_file(output_path)\n",
    "    save(fig)\n",
    "\n",
    "def compare_trajectories(out_dir: str):\n",
    "    out_dir = Path(out_dir)\n",
    "\n",
    "    vo_main_df = get_vo_df(VO_MAIN_CLOUD_TABLE)\n",
    "    vo_branch_df = get_vo_df(VO_BRANCH_CLOUD_TABLE)\n",
    "\n",
    "    vo_main_trajectories = {\n",
    "        run_id: sensorpose_to_trajectory3d(group)\n",
    "        for run_id, group in vo_main_df.groupby('run_id')\n",
    "    }\n",
    "\n",
    "    vo_branch_trajectories = {\n",
    "        run_id: sensorpose_to_trajectory3d(group)\n",
    "        for run_id, group in vo_branch_df.groupby('run_id')\n",
    "    }\n",
    "\n",
    "    for run_id in vo_main_trajectories:\n",
    "        vo_main_traj = vo_main_trajectories[run_id]\n",
    "        vo_branch_traj = vo_branch_trajectories[run_id]\n",
    "\n",
    "        plot_trajectory_alignment(vo_main_traj, vo_branch_traj, out_dir / f\"vo_{run_id}.html\")\n",
    "\n",
    "compare_trajectories(OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
